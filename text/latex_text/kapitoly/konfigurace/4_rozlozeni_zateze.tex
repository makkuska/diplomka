\subsection{Rozložení zátěže mezi replikační servery}

Doposud je samotné replikační rešení nastaveno tak, že se uživatelé musí
přihlašovat vždy ke konkrétnímu serveru v clusteru. To není úplně vhodné řešení,
protože v~případě, že se na jeden uzel, například v rámci cvičení, připojí velký
počet klientů, může se stát, že se tento uzel přetíží, zatímco ostatní uzly
nebudou plně vytížené. Zároveň by pro plynulý běh databázového clusteru bylo
vhodné, aby se příkazy pro čtení (SELECT), vykonávaly na slave serverech,
zatímco příkazy, které modifikují data (CREATE, INSERT, DELETE), na master
serveru.

Oba výše diskutované problémy řeší nástroj pgpool, který zde bude
použit v módu \texttt{master/slave} a který zajistí, že se všechny příkazy
provádějící změnu v databázi pošlou na master server a ostatní dotazy budou
rozloženy mezi slave servery. Dotazy na master server bohužel nemohou být
rozloženy, protože v replikačním clusteru smí být vždy jen jeden server s právem
pro zápis. Toto řešení zajistí také zvýšení dostupnosti dat, protože tím, že
bude mít uživatel přístup ke všem serverům místo pouze jednoho, nebude omezen v
případě výpadku kteréholi uzlů v replikačním clusteru.

Konfigurace pgpool se skládá ze tří hlavních souborů:
\begin{itemize}
  \item \texttt{pcp.conf}, který nastavuje přístupové jméno a heslo pro
    pgpool administrátora,
  \item \texttt{pool\_hba.conf}, který povoluje přístupy k pgpool pro
    konkrétní uživatele, soubor je podobný souboru \texttt{pg\_hba.conf} v
    konfiguraci PostgreSQL a
  \item \texttt{pgpool.conf}, který zajišťuje obecné nastavení.
\end{itemize}

Nejdříve je potřeba nastavit heslo pro administrátora, který bude moci měnit
nastavení a sledovat statistiky. Uživatelské jméno a heslo lze je třeba nastavit
v souboru \texttt{pcp.conf}, přičemž heslo musí být zahashováno algoritmem md5.
Tento hash lze získat pomocí utility \texttt{pg\_md5}.

Příklad zahashování hesla \uv{\texttt{kgigis}} pomocí \texttt{pg\_md5}:
%\begin{lstlisting}[identifierstyle=\color{black},stringstyle=\color{black},keywordstyle=\color{black}]
\begin{lstlisting}[language=ruby,morekeywords={pg_md5}]
> pg_md5 kgigis
eea831dcf9dc85ace5836024f3a253e7
\end{lstlisting}

Přidání přihlašovacích údajů pro administrátora do souboru \texttt{pcp.conf}:
%\begin{lstlisting}[identifierstyle=\color{black},stringstyle=\color{black},keywordstyle=\color{black}]
\begin{lstlisting}[language=ruby]
#username:[password encrypted in md5]
kgi:eea831dcf9dc85ace5836024f3a253e7
\end{lstlisting}

Zvolený \texttt{master/slave} mód počítá s již nastavenou replikací a podporuje
jak streaming replikaci, tak Slony-I. Pro usnadnění konfigurace poskytuje
pgpool příklady různých typů nastavení konfiguračních souborů. Pro
\texttt{master/slave} mód jsou připraveny hned dva vzorové soubory,
\texttt{pgpool.conf.sample-master-slave} pro Slony-I a
\texttt{pgpool.conf.sample-stream} pro streaming replikaci. Šablonu je potřeba
nejdříve přesunout do složky s konfiguračními soubory a poté přejmenovat na
\texttt{pgpool.conf}. Šablona pro mód \texttt{master/slave} replikaci nastaví
parametry:
\begin{itemize}
  \item \texttt{replication\_mode}, který povoluje replikaci, výchozí hodnota je
    \texttt{off},
  \item \texttt{load\_balance\_mode}, který povoluje rozložení zátěže, výchozí
    hodnota je \texttt{off},
  \item \texttt{master\_slave\_mode}, který povoluje propojení master a slave
    serverů,
  \item \texttt{master\_slave\_sub\_mode}, který udává, zda jde o streaming
    replikaci (hodnota \texttt{'stream'}), či Slony-I replikaci (hodnota
    \texttt{'slony'}),
  \item \texttt{sr\_check\_period}, který nastavuje, jak často má systém
    zkontrolovat pozici v~XLOGu, aby zjistil, jestli je zpoždění příliš vysoké,
    či nikoli a
  \item \texttt{delay\_threshold}, který definuje maximální možné zpoždění slave
    za master serverem, menší zpoždění je možno nastavit v případě, že je
    potřeba, aby replikace proběhla velice rychle (hodnota je určena v bytech).
\end{itemize}

Část konfiguračního souboru \texttt{pgpool.conf} s nastavením hodnot výše
popsaných parametrů:
%\begin{lstlisting}[identifierstyle=\color{black},stringstyle=\color{black},keywordstyle=\color{black}]
\begin{lstlisting}[language=ruby]
replication_mode = off
load_balance_mode = on
master_slave_mode = on
master_slave_sub_mode = 'stream'
sr_check_period = 10
log_standby_delay = 'if_over_threshold'
delay_threshold = 10000000
\end{lstlisting}

Další část konfiguračního souboru přidává konkrétní uzly, ke kterým bude možno
přistupovat přes pgpool. Pro uživatele se v podstatě nic nezmění, k databázi se
připojí stejně, jako by se přihlašovali přímo, s jediným rozdílem, že použijí
port definovaný parametrem \texttt{port}. Číslo za parametrem začínájícím
\texttt{backend} vždy značí číslo daného uzlu přidaného do pgpool. V tomto
případě jsou přidány tři uzly, kterým byla přiřazena čísla 0 pro master, 1 pro
slave1, 2 pro slave2. Konfigurační soubor \texttt{pgpool.conf} tedy dále
definuje parametry:
\begin{itemize}
  \item \texttt{listen\_addresses}, který nastavuje IP adresy, na kterých pgpool
    naslouchá,
  \item \texttt{port} určující port, na který se uživatelé budou přihlašovat k
    databázovému clusteru (místo často používaného 5432),
  \item \texttt{pcp\_port}, který stanovuje port, na který se bude přihlašovat
    administrátor,
  \item \texttt{backend\_hostname} nastavující hostname nebo IP adresu daného uzlu,
  \item \texttt{backend\_port} určující port, na kterém daný uzel naslouchá,
  \item \texttt{backend\_weight}, který umožňuje nastavit danému uzlu zátěž, čím
    vyšší číslo, tím více dotazů bude směřováno na tento uzel místo,
  \item \texttt{backend\_data\_directory}, kerý určuje, kde jsou uložená data
    daného uzlu a
  \item \texttt{backend\_flag}, který povoluje, resp. zakazuje použít daný uzel
    jako master v~případě výpadku master serveru.
\end{itemize}

Čast konfigurace souboru \texttt{pgpool.conf}, která definuje jednotlivé uzly:
\begin{lstlisting}[language=ruby]
listen_addresses = '*'
port = 9999
pcp_port = 9898

# node0 - master server
backend_hostname0 = '192.168.1.100'
backend_port0 = 5432
backend_weight0 = 2
backend_data_directory0 ='/var/lib/postgresql/9.3/main'
backend_flag0 = 'ALLOW_TO_FAILOVER'

# node1 - slave1
backend_hostname1 = '192.168.1.101'
backend_port1 = 5432
backend_weight1 = 2
backend_data_directory1 = '/var/lib/postgresql/9.1/main'
backend_flag1 = 'ALLOW_TO_FAILOVER'

# node2 - slave2
backend_hostname2 = '192.168.1.102'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/var/lib/postgresql/9.1/main'
backend_flag2 = 'ALLOW_TO_FAILOVER'
\end{lstlisting}

Pro správný běh pgpool je potřeba, aby měl slave server, který má hodnotu
\texttt{backend\_flag} nastavenou na \texttt{ 'ALLOW\_TO\_FAILOVER'}, v
souboru \texttt{recovery.conf} nastaven parametr \texttt{trigger\_file} již
popisovaný v kapitole \odkazKapitola{kStreaming}.

Následuje spuštění démona \texttt{pgpool}, kde parametr \texttt{-f} udává cestu
ke konfiguračnímu souboru \texttt{pgpool.conf} a \texttt{-F} cestu k
\texttt{pcp.conf}:
\begin{lstlisting}[language=ruby,keywords={stop,pgpool}]
> pgpool -f /etc/pgpool2/pgpool.conf -F /etc/pgpool2/ pcp.conf
\end{lstlisting}
Obdobně lze démona zastavit pomocí parametru \texttt{stop}:
\begin{lstlisting}[keywords={stop,pgpool}]
> pgpool -f /etc/pgpool2/pgpool.conf -F /etc/pgpool2/ pcp.conf stop
\end{lstlisting}

Když démon běží, je možno zkontrolovat, zda jsou všechny uzly správně nastaveny.
Lze použít utilitu \texttt{pcp\_node\_count}, která vypíše počet aktuálně
přidaných uzlů a~která vyžaduje zadání parametrů v pořadí:
\begin{itemize}
  \item \texttt{timeout}, který určí maximální čas v~sekundách, po který se má
    snažit o~vykonání příkazu
  \item \texttt{hostname}, který definuje hostname nebo  IP, na které pgpool
    naslouchá,
  \item \texttt{port} určený pro administrátora,
  \item \texttt{username} odpovídající uživatelskému jménu zadanému v
    \texttt{pcp.conf} a
  \item \texttt{password} odpovídající heslu definovanámu v \texttt{pcp.conf}.
\end{itemize}

Ukázka spuštění nástroje \texttt{pcp\_node\_count} s maximálním časem provedení
30 sekund, pgpool běžícím na localhostu, portem 9898, uživatelem \texttt{kgi} a heslem
\texttt{kgigis}:
\begin{lstlisting}[language=ruby,morekeywords={pcp_node_count}]
> pcp_node_count 30 localhost 9898 kgi kgigis
3
\end{lstlisting}

Výsledkem jsou tři aktuálně běžící servery. Obdobně lze získat informace
o~konkrétních uzlech utilitou \texttt{pcp\_node\_info}, která navíc přidává
parametr \texttt{nodeID}, který odpovídá ID uzlu, o kterém chceme získat
informace.

Ukázka spuštění nástroje \texttt{pcp\_node\_info} pro uzly 0, 1 a 2 s maximálním
časem provedení 30 sekund, pgpool běžícím na localhostu, portem 9898, uživatel
\texttt{kgi} a heslem \texttt{kgigis}:
\begin{lstlisting}[language=ruby,morekeywords={pcp_node_info}]
> pcp_node_info 30 localhost 9898 kgi kgigis 0
192.168.1.100 5432 1 0.400000
> pcp_node_info 30 localhost 9898 kgi kgigis 1
192.168.1.101 5432 1 0.400000
> pcp_node_info 30 localhost 9898 kgi kgigis 2
192.168.1.102 5432 1 0.200000
\end{lstlisting}

Z výpisu lze vidět na kterých IP adresách a portech dané uzly běží. Poslední
informace ukazuje míru vytížení serveru podle toho, jak byla u daných uzlů
nastavena hodnota \texttt{backend\_weight}. pgpool nabízí ještě další nástroje,
např. \texttt{pcp\_pool\_info}, pomocí něhož lze získat informace o nastavení
pgpool, nebo \texttt{pcp\_promote\_node} pro změnu uzlů z master na slave a
opačně\footnote{kompletní seznam nástrojů pgpool na
\url{http://www.pgpool.net/docs/latest/pgpool-en.html\#pcp\_command}}.
