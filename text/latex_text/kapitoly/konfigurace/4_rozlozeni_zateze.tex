\subsection{Rozložení zátěže mezi replikační servery}

Doposud je samotné replikační rešení nastaveno tak, že se uživatelé musí přihlašovat vždy ke konkrétnímu serveru v clusteru. To není úplně vhodné řešení, protože v případě, že se na jeden uzel, například v rámci cvičení, připojí velký počet klientů, může se stát, že se daný server přetíží, zatímco druhý slave server nezaznamená žádnou zátěž. Zároveň by pro plynulý běh databázového clusteru bylo vhodné, aby se uživatelé s dotazy SELECT, připojovali pouze na slave servery, zatímco příkazy, které modifikují data (CREATE, INSERT, DELETE), byly vykonány na master serveru. 

Oba výše diskutované problémy řeší nástroj \texttt{pgpool}, který zde bude použit v módu \texttt{master/slave} a který zajistí, že se všechny příkazy provádějící změnu v databázi pošlou na master server a ostatní dotazy budou rozloženy mezi slave servery. Dotazy na master server bohužel nemohou být rozloženy, protože v replikačním clusteru smí být vždy jen jeden server s právem pro zapis. Toto řešení zajistí také zvýšení dostupnosti dat, protože tím, že bude mít uživatel přístup ke všem serverům místo pouze jednoho, nebude omezen v případě výpadku kteréholi uzlů v replikačním clusteru.

    Konfigurace pgpool se skládá ze tří hlavních souborů:
\begin{itemize}
  \item \texttt{pcp.conf}, který nastavuje přístupové jméno a heslo pro \texttt{pgpool} administrátora, 
  \item \texttt{pool\_hba.conf}, který povoluje přístupy k \texttt{pgpool} pro konkrétní uživatele, soubor je podobný souboru \texttt{pg\_hba.conf} v konfiguraci PostgreSQL a 
  \item \texttt{pgpool.conf}, který zajišťuje obecné nastavení. 
\end{itemize}

Nejdříve je potřeba nastavit heslo pro administrátora, který bude moct měnit nastavení a sledovat statistiky. Heslo lze vytvořit pomocí ulitity \texttt{pg\_md5}, jehož výstupem je zašifrované heslo, které je poté potřeba zkopírovat do souboru \texttt{pcp.conf} a doplnit jej o uživatelské jméno.

Příklad zašifrování hesla \texttt{kgigis} pomocí \texttt{pg\_md5} s vypsaným výsledkem:
%\begin{lstlisting}[identifierstyle=\color{black},stringstyle=\color{black},keywordstyle=\color{black}]
\begin{lstlisting}[language=ruby,morekeywords={pg_md5}]
> pg_md5 kgigis
eea831dcf9dc85ace5836024f3a253e7
\end{lstlisting}

Přidání přihlašovacích údajů pro administrátora do souboru \texttt{pcp.conf}:
%\begin{lstlisting}[identifierstyle=\color{black},stringstyle=\color{black},keywordstyle=\color{black}]
\begin{lstlisting}[language=ruby]
#username:[password encrypted in md5]
kgi:eea831dcf9dc85ace5836024f3a253e7
\end{lstlisting}

Zvolený \texttt{master/slave} mód počítá s již nastavenou replikací a podporuje jak streaming replikaci, tak Slony-I. Pro usnadnění konfigurace pgpool poskytuje příklady všech konfiguračních souborů včetně různých typů nastavení. Pro \texttt{master/slave} mód jsou připraveny hned dva příkladové soubory, \texttt{pgpool.conf.sample-master-slave} pro Slony-I a \texttt{pgpool.conf.sample-stream} pro streaming replikaci. V případě použití této šablony, je nejdříve potřeba přesunout ji do složky s konfiguračními soubory a poté přejmenovat na \texttt{pgpool.conf}. Šablona zajistí základní konfiguraci pro mód \texttt{master/slave} a streaming replikaci, nastaví tedy parametry:
\begin{itemize}
\item \texttt{replication\_mode}, který povoluje replikaci, výchozí hodnota je off, 
\item \texttt{load\_balance\_mode}, který umožňuje rozložení zátěže, východí hodnota je off,
\item \texttt{master\_slave\_mode}, který povoluje propojení master a slave serverů, 
\item \texttt{master\_slave\_sub\_mode}, který nastavuje hodnotu na \texttt{'stream’} v případě streaming replikace a \texttt{'slony'} v případě Slony-I, 
\item \texttt{sr\_check\_period}, který nastavuje, jak často má systém zkontrolovat pozici v XLOGu, aby zjistil, jestli je zpoždění příliš vysoké, či nikoli a 
\item \texttt{delay\_threshold}, který definuje maximální možné zpoždění slave za master serverem, menší zpoždění je možno nastavit v případě, že je potřeba, aby replikace proběhla velice rychle (hodnota je určena v bytech). 
\end{itemize}

Část konfiguračního souboru \texttt{pgpool.conf} s nastavením hodnot výše popsaných parametrů:
%\begin{lstlisting}[identifierstyle=\color{black},stringstyle=\color{black},keywordstyle=\color{black}]
\begin{lstlisting}[language=ruby]
replication_mode = off	
load_balance_mode = on	
master_slave_mode = on	
master_slave_sub_mode = 'stream'
sr_check_period = 10
log_standby_delay = 'if_over_threshold'
delay_threshold = 10000000 
\end{lstlisting}

Další část konfiguračního souboru přidává konkrétní uzly, ke kterým bude možno přistupovat přes pgpool. Pro uživatele se v podstatě nic nezmění, k databázi se připojí stejně, jako by se přihlašovali přímo, s jediným rozdílem, že použijí port definovaný v tom souboru parametrem \texttt{port}. Číslo za parametrem začínájícím \texttt{backend} vždy značí číslo daného nodu přidaného do pgpool. V tomto případě jsou přidány tři uzly, kterým byla přiřazena čísla 0 pro master, 1 pro slave1, 2 pro slave2. Konfigurační soubor \texttt{pgpool.conf} tedy dále definuje parametry: 
\begin{itemize}
\item \texttt{listen\_addresses}, který nastavuje IP adresy, na kterých pgpool naslouchá, 
\item \texttt{port} určující port, kterým se uživatelé budou přihlašovat k databázovému clusteru (místo často používaného 5432), 
\item \texttt{pcp\_port}, který stanovuje port, kterým se bude přihlašovat administrátor, 
\item \texttt{backend\_hostname} nastavující hosta nebo IP adresu daného uzlu, 
\item \texttt{backend\_port0} určující port, na kterém daný uzel naslouchá, 
\item \texttt{backend\_weight}, který umožňuje zvýšit danému uzlu zátěž, čím vyšší číslo, tím více datazů bude směřováno na tento uzel místo, 
\item \texttt{backend\_data\_directory}, kerý určuje, kde jsou uložená data daného uzlu a 
\item \texttt{backend\_flag}, který povoluje, resp. zakazuje použít daný uzel jako master v případě výpadku master serveru.
\end{itemize}

Čast konfigurace souboru \texttt{pgpool.conf}, která definuje jednotlivé uzly:
\begin{lstlisting}[language=ruby]
listen_addresses = '*'
port = 9999 		
pcp_port = 9898 	

# node 0 - master server
backend_hostname0 = '192.168.1.100' 			
backend_port0 = 5432 					
backend_weight0 = 2					
backend_data_directory0 ='/var/lib/postgresql/9.3/main'
backend_flag0 = 'ALLOW_TO_FAILOVER'	      

# node1 - slave1
backend_hostname1 = '192.168.1.101'
backend_port1 = 5432
backend_weight1 = 2
backend_data_directory1 = '/var/lib/postgresql/9.1/main'
backend_flag1 = 'ALLOW_TO_FAILOVER'

# node2 - slave2
backend_hostname2 = '192.168.1.102'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/var/lib/postgresql/9.1/main'
backend_flag2 = 'ALLOW_TO_FAILOVER'
\end{lstlisting}

Pro správný běh pgpool je potřeba, aby slave server, který má hodnotu \texttt{backend\_flag} nastavenou na \texttt{ 'ALLOW\_TO\_FAILOVER'}, měl v souboru \texttt{recovery.conf} přidán parametr \texttt{trigger\_file} již popisovaný v kapitole \odkazKapitola{kStreaming}.

Následuje spuštění démona \texttt{pgpool}, kde \texttt{f} uvádí cestu ke konfiguračnímu souboru \texttt{pgpool.conf} a \texttt{F} cestu k \texttt{pcp.conf}:
\begin{lstlisting}[language=ruby,keywords={stop,pgpool}]
> pgpool -f /etc/pgpool2/pgpool.conf -F /etc/pgpool2/ pcp.conf
\end{lstlisting}

Obdobně lze démona zastavit pomocí klíčového slova \texttt{stop}:
\begin{lstlisting}[keywords={stop,pgpool}]
> pgpool -f /etc/pgpool2/pgpool.conf -F /etc/pgpool2/ pcp.conf stop
\end{lstlisting}

Když démon běží, je možno zkontrolovat, zda jsou všechny uzly správně. Lze použít utilitu \texttt{pcp\_node\_count}, která vypíše počet aktuálně přidaných uzlů. Zadání dále vyžaduje definici parametrů v pořadí:
 \begin{itemize}
\item \texttt{timeout}, který učí maximální čas, po který se má snažit o vykonání příkazu v sekundách,
\item \texttt{hostname}, který definuje host nebo  IP, na které pgpool naslouchá,
\item \texttt{port} definovaný pro administrátora,
\item \texttt{username} odpovídající uživatelskému jménu zadanému v pcp.conf a 
\item \texttt{password} odpovídající heslu definovanámu v pcp.conf.
\end{itemize}

Ukázka spuštění nástroje \texttt{pcp\_node\_count} s maximálním časem provedení 30 sekund, pgpool běžícím na localhostu, portem 9898, uživatel kgi a heslem kgigis:
\begin{lstlisting}[language=ruby,morekeywords={pcp_node_count}] 
> pcp_node_count 30 localhost 9898 kgi kgigis
3
\end{lstlisting}

Výsledkem jsou tři aktuálně běžící servery. Obdobně lze získad informace o konkrétních uzlech utilitou \texttt{pcp\_node\_info}, která navíc přidává parametr \texttt{nodeID}, který odpovídá ID uzlu, o kterém chceme získat informace.

Ukázka spuštění nástroje \texttt{pcp\_node\_info} pro uzly 0, 1 a 2 s maximálním časem provedení 30 sekund, pgpool běžícím localhostu, portem 9898, uživatel kgi a heslem kgigis:
\begin{lstlisting}[language=ruby,morekeywords={pcp_node_info}]
> pcp_node_info 30 localhost 9898 kgi kgigis 0
192.168.1.100 5432 1 0.400000
> pcp_node_info 30 localhost 9898 kgi kgigis 1
192.168.1.101 5432 1 0.400000
> pcp_node_info 30 localhost 9898 kgi kgigis 2
192.168.1.102 5432 1 0.200000
\end{lstlisting}

Z výpisu lze vidět na kterých IP adresách a portech dané uzly běží. Poslední informace ukazuje míru vytížení serveru podle toho, jak u daných uzlů byla nastavena hodnota \texttt{backend\_weight}. pgpool nabízí ještě další nástroje, pomocí který lze získat informace o nastavení pgpool \texttt{pcp\_pool\_info} nebo \texttt{pcp\_promote\_node} pro změnu uzlů z master na slave a opačně\footnote{kompletní seznam nástrojů pgpool na \url{http://www.pgpool.net/docs/latest/pgpool-en.html\#pcp\_command}}.

